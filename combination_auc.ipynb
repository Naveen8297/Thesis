{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa8d68dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5354540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0c42e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a22b5990",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pow = [2, 1, 0, -1, -2, -3, -4, -5, -6, -7, -8]\n",
    "targets = [10**x for x in target_pow]     #set of target values for ecdf calculation\n",
    "evals = [1, 223, 445, 668, 890, 1112, 1334, 1557, 1779, 2001, 2223, 2445, 2668, 2890,\n",
    "         3112, 3334, 3557, 3779, 4001, 4223, 4445, 4668, 4890, 5001]   #budget values at which ecdf will be calulated\n",
    "                                                                       #set these according to csv files of original target samples obtained from iohanalyzer\n",
    "algorithms = ['CMA','DiagonalCMA','PSO','NGO','Shiwa','EDA','NelderMead','NaiveIsoEMNA','DE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14fec584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all possible combinations to be run (simulated) in parallel\n",
    "def algo_combination(algorithms=algorithms):    \n",
    "    rem = []\n",
    "    cross = list(itertools.product(algorithms, algorithms))\n",
    "    for s in cross:\n",
    "        if s[0]==s[1]:\n",
    "            continue\n",
    "        if s not in rem and (s[1],s[0]) not in rem:\n",
    "            rem.append(s)\n",
    "    return rem  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1adcc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = list(np.arange(25, 44))  #function ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22c5932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_single_set = pd.DataFrame(columns=algorithms, index=functions)   #single algorithms, single set of 25 runs for each algo\n",
    "df_single_set = df_single_set.fillna(0)\n",
    "df_single_set = df_single_set.astype(float)\n",
    "#df_single_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4aa569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#average ecdf auc calculation for single algorithms, single set of 25 runs for each algo\n",
    "def single_set_auc(func, algo, targets=targets, evals=evals):\n",
    "    df = pd.read_csv(r'filepath/FVSample-10DF'+str(func)+'.csv')  #csv files of original target samples for each fn obtained by\n",
    "                                                                    #uploading logger data to iohanalyzer\n",
    "    pts_algo = []\n",
    "    for feval in evals:\n",
    "        tar_tot = 0\n",
    "        \n",
    "        for target in targets:\n",
    "            total = 0\n",
    "            \n",
    "            for i in range(4, 29):     #25 runs\n",
    "                if float(df[df['ID']==algo][df['runtime']==feval].iloc[:, i]) <= target:\n",
    "                    total += 1\n",
    "                \n",
    "            total = total/25   #average over 25 runs\n",
    "            tar_tot += total\n",
    "            \n",
    "        tar_tot = tar_tot/len(targets)    #average over set of targets\n",
    "        pts_algo.append(tar_tot)\n",
    "        \n",
    "    area = auc(evals, pts_algo)\n",
    "    df_single_set.loc[func][algo] = area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0d8c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_single_algo = pd.DataFrame(columns=algorithms, index=functions)   #single algorithms, better values of the two sets of runs for each algo\n",
    "df_single_algo = df_single_algo.fillna(0)\n",
    "df_single_algo = df_single_algo.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bed72acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to calculate combined auc for set1, set2 of the same algorithm (better values out of the two sets) \n",
    "def set1_set2_auc(func, algo, targets=targets, evals=evals):\n",
    "    df1 = pd.read_csv(r'filepath1/FVSample-10DF'+str(func)+'.csv')  #set-1, 25 runs\n",
    "    df2 = pd.read_csv(r'filepath2/FVSample-10DF'+str(func)+'.csv')  #set-2, 25 runs\n",
    "   \n",
    "    pts_algo1 = []\n",
    "    pts_algo2 = []\n",
    "    \n",
    "    for feval in evals:\n",
    "        tar_tot1 = 0\n",
    "        tar_tot2 = 0\n",
    "        for target in targets:\n",
    "            total1 = 0\n",
    "            total2 = 0\n",
    " \n",
    "            for i in range(4, 29):\n",
    "                if float(df1[df1['ID']==algo][df1['runtime']==feval].iloc[:, i]) <= target:\n",
    "                    total1 += 1\n",
    "                if float(df2[df2['ID']==algo][df2['runtime']==feval].iloc[:, i]) <= target:\n",
    "                    total2 += 1\n",
    "            total1 = total1/25\n",
    "            total2 = total2/25\n",
    "            tar_tot1 += total1\n",
    "            tar_tot2 += total2\n",
    "        tar_tot1 = tar_tot1/len(targets)\n",
    "        tar_tot2 = tar_tot2/len(targets)\n",
    "        pts_algo1.append(tar_tot1)\n",
    "        pts_algo2.append(tar_tot2)\n",
    "\n",
    "    area = max(auc(evals, pts_algo1), auc(evals, pts_algo2))  #better of the two sets of runs\n",
    "    df_single_algo.loc[func][algo] = area\n",
    "    print(func, algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a421db11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert column names from string to tuple\n",
    "def conv_str_tuple(df):\n",
    "    c = \"'\"\n",
    "    for col in df.columns:\n",
    "        new_col = re.sub(c, '', col)\n",
    "        df.rename(columns = {col:new_col}, inplace = True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "668328af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auc = pd.DataFrame(columns=algo_combination(), index=functions)   #algorithm combinations\n",
    "df_auc = df_auc.fillna(0)\n",
    "df_auc = df_auc.astype(float)\n",
    "#df_auc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2d31a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate combined ecdf auc for algorithm combinations\n",
    "def ecdf_auc(func, algo_tuple, targets=targets, evals=evals):    \n",
    "    \n",
    "    df = pd.read_csv(r'filepath/FVSample-10DF'+str(func)+'.csv')\n",
    "    \n",
    "    algo1 = algo_tuple[0]\n",
    "    algo2 = algo_tuple[1]\n",
    "    \n",
    "    \n",
    "    #fig, ax = plt.subplots(1, 2, figsize=(10, 4))  uncomment if plot reqd.\n",
    "    pts_algo1 = []\n",
    "    pts_algo2 = []\n",
    "    pseudo = []\n",
    "    \n",
    "    \n",
    "    for feval in evals:\n",
    "        tar_tot1 = 0\n",
    "        tar_tot2 = 0\n",
    "        for target in targets:\n",
    "            total1 = 0\n",
    "            total2 = 0\n",
    "            for i in range(4, 29):\n",
    "                if float(df[df['ID']==algo1][df['runtime']==feval].iloc[:, i]) <= target:  #for algorithm-1\n",
    "                    total1 += 1\n",
    "                if float(df[df['ID']==algo2][df['runtime']==feval].iloc[:, i]) <= target:   #for algorithm-2\n",
    "                    total2 += 1\n",
    "            total1 = total1/25\n",
    "            total2 = total2/25\n",
    "            tar_tot1 += total1\n",
    "            tar_tot2 += total2\n",
    "        tar_tot1 = tar_tot1/len(targets)\n",
    "        tar_tot2 = tar_tot2/len(targets)\n",
    "        pts_algo1.append(tar_tot1)\n",
    "        pts_algo2.append(tar_tot2)\n",
    "        \n",
    "    for i in range(len(pts_algo1)):\n",
    "        if pts_algo1[i] >= pts_algo2[i]:\n",
    "            pseudo.append((pts_algo1[i], algo1))\n",
    "        else:\n",
    "            pseudo.append((pts_algo2[i], algo2))\n",
    "     \n",
    "    pts_pseudo = [x[0] for x in pseudo]   #only higher ecdf values of the two algorithms\n",
    "   \n",
    "    ax[0].plot(evals, pts_algo1, color='blue', marker='o')\n",
    "    ax[0].plot(evals, pts_algo2, color='red', marker='^')\n",
    "    ax[0].legend([algo1, algo2], prop={'size': 9})\n",
    "    ax[0].set_title('ECDF of '+algo1+' and\\n'+algo2+' on F'+str(func), fontsize=9)\n",
    "    ax[0].set_xlabel('Function evaluations', fontsize=9)\n",
    "    ax[0].set_ylabel('Proportion of runs', fontsize=9)\n",
    "    ax[0].tick_params(axis='x', labelsize= 9)\n",
    "    ax[0].tick_params(axis='y', labelsize= 9)\n",
    "    \n",
    "    ax[1].plot(evals, pts_pseudo, color='green', marker='o', alpha=0.6)\n",
    "    area = auc(evals, pts_pseudo)\n",
    "    df_auc.loc[func][algo_tuple] = area\n",
    "    area = round(area, 2)\n",
    "        \n",
    "    ax[1].set_title('ECDF of combination on F'+str(func)+'\\nAUC: '+str(area), fontsize=9)\n",
    "    \n",
    "    ax[1].set_xlabel('Function evaluations', fontsize=9)\n",
    "    ax[1].set_ylabel('Proportion of runs', fontsize=9)\n",
    "    ax[1].tick_params(axis='x', labelsize= 9)\n",
    "    ax[1].tick_params(axis='y', labelsize= 9)\n",
    "    \n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    plt.savefig(r'graphs_filepath/'+str(func)+'_'+algo1+'_'+algo2+'.pdf')\n",
    "    print(func, algo_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba1bc6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auc = df_auc.join(df_single_algo)  #combination of 2 distinct algos + combination of two runs same algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a65f182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auc_n = df_auc/5000  #normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22d1fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot heatmap for normalized auc values\n",
    "def hmap_auc(df):\n",
    "    x = sns.heatmap(df, xticklabels=1, cmap=\"rocket_r\", linewidths=0.5, linecolor='white')\n",
    "    ax.set_xlabel('Algorithm (combinations)')\n",
    "    ax.set_ylabel('Function ID')\n",
    "    ax.set_title('Heatmap of algorithm (combination) performance on functions\\n(measured by area under ECDF curve)')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=60, horizontalalignment='right')\n",
    "    figure = plt.gcf()  # get current figure\n",
    "    #figure.set_tight_layout(True)\n",
    "    figure.set_size_inches(20,6)\n",
    "    plt.savefig('hmap.pdf', bbox_inches='tight')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30e65ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.DataFrame(columns=df_auc.columns, index=functions)  #labels dataframe reqd. for plotting improvement hmap\n",
    "df_labels = df_labels.fillna(0)\n",
    "df_labels = df_labels.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d84c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare combination performance to better performing algo/run of the two on each function\n",
    "def compare_algo(df_auc=df_auc, h=True):\n",
    "    \n",
    "    df_combo = df_auc.copy()\n",
    "    \n",
    "    for col in df_combo.columns:\n",
    "        if col[0]!='(':    #if algo is not tuple, then it is a single algo\n",
    "            single = True\n",
    "            #a1 = col\n",
    "            #a2 = col\n",
    "        else:\n",
    "            single = False\n",
    "            a1 = col.split(',')[0][1:]\n",
    "            a2 = col.split(',')[1][1:-1]   #split into respective algo names\n",
    "        for func in functions:\n",
    "            if h:\n",
    "                if single:\n",
    "                    div = df_single_set.loc[func][col]    #compare with 1 set of runs\n",
    "                    \n",
    "                else:    \n",
    "                    div = max(df_single_set.loc[func][a1], df_single_set.loc[func][a2]) #sbs for that function\n",
    "                    if div == df_single_set.loc[func][a1]:\n",
    "                        df_labels.loc[func][col] = 1     #label for sbs position (1 denotes algo1 out of (algo1,algo2) is better)\n",
    "                    else:\n",
    "                        df_labels.loc[func][col] = 2\n",
    "            \n",
    "            \n",
    "            if div==0.0:\n",
    "                div+=0.000001\n",
    "            df_combo.loc[func][col] = df_combo.loc[func][col]/div    #comparison\n",
    "    return df_combo               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f333466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hmap of performance improvement of combination w.r.t. sbs per fn\n",
    "def hmap_imp_perfn(df_combo):\n",
    "    labels = df_labels.to_numpy()\n",
    "    ax = sns.heatmap(df_combo, annot=labels, xticklabels=1, cmap=\"rocket_r\", linewidths=0.5, linecolor='white', vmin=1.0, vmax=1.10)\n",
    "    ax.set_xlabel('Algorithm (combinations)')\n",
    "    ax.set_ylabel('Function ID')\n",
    "    title = 'Heatmap of algorithm combination performance improvement with '\n",
    "    title += 'respect to\\n the single best solver of the two (for each function) on functions\\n'\n",
    "    title += '(measured by area under ECDF curve)'\n",
    "    #ax.set_ylim(0.0, 0.05)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=60, horizontalalignment='right')\n",
    "    figure = plt.gcf()  # get current figure\n",
    "    #figure.set_tight_layout(True)\n",
    "    figure.set_size_inches(20,6)\n",
    "    plt.savefig('hmap_sbs_hardline.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "becf3d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bar_plot for performance\n",
    "def bar_plot(df_auc_n=df_auc_n):\n",
    "    \n",
    "    col_sum = {}\n",
    "    for col in df_auc_n.columns:\n",
    "        col_sum[col] = df_auc_n[col].sum()   #cumulative performance across fns for each algo/combination\n",
    "    col_sum = pd.DataFrame.from_dict(col_sum, orient='index')\n",
    "    col_sum = col_sum.rename(columns={0: \"Performance\"})\n",
    "    col_sum = col_sum.sort_values(by=['Performance'])  #ascending order of performance\n",
    "    \n",
    "    x = list(np.arange(45))   #xticks for algorithms and combinations\n",
    "    col_sum = col_sum.reset_index()\n",
    "    plt.bar(x, col_sum['Performance'])\n",
    "    plt.xlabel('Algorithm combinations', fontsize=7)\n",
    "    plt.ylabel('Performance measure', fontsize=7)\n",
    "    plt.title('Overall performance of algorithms across functions', fontsize=7)\n",
    "    plt.xticks(x, col_sum['index'], rotation=90, fontsize=6)\n",
    "    plt.yticks(fontsize=7)\n",
    "    figure = plt.gcf()  # get current figure\n",
    "    #figure.set_tight_layout(True)\n",
    "    figure.set_size_inches(8,4)\n",
    "    plt.savefig(r'ranking.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac0d43a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (newtf)",
   "language": "python",
   "name": "newtf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
